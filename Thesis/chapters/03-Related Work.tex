\chapter{Related Work}
\label{related Work}

Over the years many different approaches have been adopted to solve the problem of Candidate Generation. In \cite{Sevgili2020}, \cite{8999622} and \cite{Shen2015} three common methods are identified; methods that are based on
\begin{itemize}
\item{(1) surface form matching,}
\item{(2) expansion with aliases, and surface form processing in general,}
\item{(3) prior matching probability computation.}\newline
\end{itemize}

\section{Surface form matching}
In the first method, after extracting the entity's mention from the input text, its surface form is compared to strings that are references for the entity. These strings can be gathered from multiple sources. Because comparing a mention's form to every entity in a knowledge base, which usually contains in the order of millions of entities, is not a sustainable approach nor a scalable one, a common approach to implementing this method is to first build either an index or a dictionary (\textit{name dictionaries}) mapping groups of possible words or expressions to single entities or vice versa.\newline

\subsection{Index/Dictionary compilation}
\label{dictsubsection}
Indices and dictionaries are used because they are data structures that organize huge amounts of data in a way such that the retrieval time is considerably short.\footnote{https://starship-knowledge.com/index-data-structures}\footnote{https://wiki.python.org/moin/TimeComplexity}\newline
Dictionaries describe an unordered collection of data in a set of key-value pairs, and each key must be unique \footnote{https://isaaccomputerscience.org/concepts/dsa\_datastruct\_dictionary?examBoard=all\&stage=all}. In contrast, indices are more complex and store the data in a certain order. The order varies between the type of indices and depends on the mathematical properties, which are employed during the construction of the index\footnote{https://starship-knowledge.com/index-data-structures}.\newline

\indent For example, in \cite{AGDISTIS} an index is firstly built by retrieving all labels for a given entity. These labels are properties of the entity such as \textbf{rdfs:label}, and different surface forms retrieved from the published lexicalization dataset computed in \cite{Mendes2011} as part of the DBpedia spotlight project if any were available. The latter surface forms were collected from the DBpedia graph of labels, redirects, and disambiguations \cite{Mendes2011}.
This method of leveraging Wikipedia's redirect and disambiguation pages is widely adopted as well by various other works to build name dictionaries, \cite{Phan2017}, \cite{bunescu-pasca-2006-using},  \cite{zhang2010entity}, \cite{cucerzan2007large}. Most of these works, in addition to disambiguation and redirect pages, add to the dictionaries page titles as well as anchor texts.\newline
In \cite{Moussallem2017} instead of relying on one index, they chose to compile different indices for different cases. A first index, for surface forms which in this case did not only save the entity's label but its type as well (\textbf{rdfs: type}). A second index for person names, this index takes into consideration the differences in names used to refer to people across languages and domains. For entities, for which written descriptions are available (\textbf{rdfs:comment}), a third index is constructed. To build the latter index, i.e. Rare references index, noun phrases that contain adjectives are collected from the first line in the description. Pertaining to surface forms one last index is used, namely, an Acronyms index, which was handcrafted from STANDS4\footnote{http://www.abbreviations.com/}\newline
Other works rely on different methods for compiling such mappings (\textit{entity url} to \textit{raw text}). \cite{spitkovsky-chang-2012-cross} created a dictionary in which entries were generated by crawling the web in search of short natural language strings that can be associated with a given English Wikipedia URL. These strings are primarily 
\begin{itemize}
\item{\textit{(i)} English Wikipedia titles;}
\item{\textit{(ii)} anchor texts from English inter-Wikipedia links;}
\item{\textit{(iii)} anchor texts into the English Wikipedia from non-Wikipedia web-pages;}
\item{and \textit{(iv)} anchor tets from non-Wikipedia pages into non-Eglish Wikipedia pages.}\newline
\end{itemize}

It is worth mentioning here that some works do not compile these mappings but rather rely on search engines directly. In \cite{Han2009NLPR}, \cite{Durrett2014}, and \cite{Fang2020} the mention as well as variations thereof is submitted to the Wikipedia search engine.\newline \cite{Han2009NLPR} and \cite{Fang2020} even extend this, in the case of not finding enough results, with a submission to Google's API, to account for misspellings and other reasons that might lead to a lack of search results from the Wikipedia search engine.

\subsection{Surface forms matching}
\label{Surface forms matching}
After compiling the indices and the dictionaries the retrieval of candidates in real-time becomes feasible, for now, mentions surface form can be retrieved in a very short time. To this end, there exists a multitude of string comparison methods.\cite{AGDISTIS} together with \cite{Parravicini2019}, \cite{Moussallem2017} and \cite{Phan2017} implement the n-gram similarity metric, with different values for n to retrieve candidates.\newline
\cite{Phan2017} and others, like, \cite{Moreno2017}, test for equality of forms between the mention and the entity. In addition, they check if the surface form detected, is a substring of the entity, and compute the Levenstein distance to measure the similarity between the mention and variations of the entity's form. Nevertheless, they do not limit the search to the initial mention's form detected, we will explore how they introduce the second method named above (i.e. expansion with aliases and surface form processing) in the subsequent section \ref{surface form processing}.

\section{Expansion with aliases and surface form processing}
\label{surface form processing}
In this section, we go over the distinct strategies implemented to process (e.g. normalize, replace, extend, remove, ...) the reference of an entity in the input text, such that, the set of query strings is expanded and more candidates can be fetched from the indices or dictionaries built.\newline

Most of the works cited in the previous subsection \ref{Surface forms matching} do not merely use the primary surface forms detected. Knowing that a single entity can have a plethora of words or expressions that refer to it and in turn, these mentions themselves can have as well numerous ways in which they can be written, we can deduce on one hand, that just using the detected form as a focal point for the search can result in few if any results. Similarly, if we take into consideration the fact that one single reference can be polysemic/homonymous\footnote{Words that have similar spelling or pronunciation but different meanings; (in this context we refer to words with similar spelling.)}, we can deduce, on the other hand, that the search might lead to scattered results, concerning the meaning.\newline
There are a lot of other reasons that will require the expansion of the query set and on account of these challenges, string manipulation techniques are implemented and these vary from one work to the other.\newline

\cite{AGDISTIS}, for example, utilized string normalization techniques. These consist of eliminating genitives and plural forms of the mention, removing affixes, and eliminating mentions that contain time information. Other natural language processing techniques used for form expansion include; Lemmatization, true casing utilized in \cite{Durrett2014}; removal of punctuation and additional white space, as well as camel casing recognition in \cite{Moussallem2017}.
An additional policy that the system AGDISTIS in \cite{AGDISTIS} implemented, which is also adapted by \cite{pershina-etal-2015-personalized}, is the expansion of the surface form of the mention (a task of co-reference resolution). This policy translates into replacing subsequent mentions present in the text with the first longest mention detected that refers to the same entity. So for example, if in a text the mention \textit{Barack Obama} is detected, succeeding mentions, which are substrings of the latter name, e.g. \textit{Barack} or \textit{Obama} will be mapped to it.\newline

In \cite{Fang2020} they expand the search query set in multiple ways. An external resource, WordNet\footnote{A Lexical Database for English https://wordnet.princeton.edu/} is used to retrieve synonyms of the mention detected that will be used as queries. Additionally, and along with \cite{Han2009NLPR}, to ensure context dependency, keywords from the context are also inserted into the set of queries. Moreover, to further consider meaning derived from context, they submit besides the mention, other adjacent mentions, thus leveraging information that can be extracted from the co-occurrence principle.\newline

\cite{Phan2017} use mention boundary correction, in \cite{Lehmann2010LCCAT} this method was named soft mention, this approach translates into parsing the local context of a mention in order to detect if the words near it can also be appended to the surface form. In \cite{Phan2017} an off-the-shelf entity recognizer was employed and in \cite{Lehmann2010LCCAT} they expanded the form if the string has a high dice coefficient.\newline

The consideration of abbreviations and acronyms is also a broadly utilized technique. As mentioned before in \cite{Moussallem2017} a separate index was generated for these cases. In \cite{SFrefinement} they implemented a module for surface form detection which generates abbreviations automatically. This module also generates alternative surface forms (e.g. by adding 's' at the end of the word) and by reordering n-grams. A second module that they implemented, in case of a lack of results from the first one, is for surface form correction. At this stage they set three rules to take into consideration the alternative form suggested, namely if the suggestion and the mention's surface form have at least one common word; if the lexical distance between the two is equal to one (example: "\textit{Michicgan}" to "\textit{Michigan}"); or if the edit distance between the two if the first letter is similar, is lower than a certain threshold. With this approach, misspellings would also be accounted for. Another strategy to handle these exceptions (i.e. misspelling and others), was proposed by \cite{Fang2020} and \cite{Han2009NLPR} as well as others; is the submission of the form to a search engine (e.g. Lucene, Google, ...).\newline

On top of these approaches, there are also precomputed dictionaries. A well-known one is the YAGO dictionary first introduced in \cite{YAGO} and extended in \cite{YAGO2} (YAGO2). One of the relations introduced in YAGO2 is the "\textit{means}" relationship, which offers a wide range of alternative names, including some that are multilingual. Many works make use of this resource to expand the search query for candidate entities (\cite{Sevgili2020}).

\section{Prior matching probability computation}
Prior matching probability is the matching of mentions to entities based on the probability that a certain entity \textit{e} is the one being referred to in the input text, given a mention \textit{m}. When expressed in mathematical terms, it can be written as $P(\textit{e} \mid \textit{m})$. The uses for this probability vary between different works \cite{Sevgili2020}.\newline

The dictionary compiled in \cite{spitkovsky-chang-2012-cross} and mentioned in \ref{dictsubsection}, on top of saving the mapping between strings and Wikipedia concepts, stores some additional information about the conditional probability in two ways. The first is the probability of an entity \textit{e} being referenced given a mention \textit{m} $P(\textit{e} \mid \textit{m})$. It is computed by the ratio of the number of hyperlinks into the Wikipedia article \textit{URL} having anchor text \textit{s}  to the total number of anchor texts with string \textit{s} found during the web crawl. And the second is the probability of the mention \textit{m} referencing a certain entity \textit{e} $P(\textit{m} \mid \textit{e})$. It is represented by the ratio of the number of hyperlinks into the Wikipedia article \textit{URL} having anchor text \textit{s} to the total count of links pointing to that same article. Both the works of \cite{Gupta2017} and \cite{Ganea2017} rely on this scoring function to generate the candidate sets.
Other works that compute these probabilities, like, \cite{ratinov-etal-2011-local} \cite{hoffart-etal-2011-robust}, usually rely solely on Wikipedia anchor texts, however, \cite{ling-etal-2015-design} have shown that CrossWiki dictionaries lead to better prior scores and candidates recall. \newline

The scores computed in these dictionaries can also be used to prune the set of candidates \cite{Fang2020}. Some research has shown that reducing the number of candidates to a small subset does not affect performance as much as it improves efficiency \cite{Gupta2017}.

\section{Embeddings}
Next to three strategies named previously in this section, a newer one has been introduced to generate candidate entities by \cite{Gillick2019}. This strategy has shown some promising results and even surpassed some of the methods that rely on prior probability matchings. Instead of comparing syntactic forms, embeddings for the mentions and the entities are computed and based on these dense compiled vectors, a scoring function determines the similarity of entity mention pairs. These scores are then used to retrieve a subset of the top \textit{k} most similar entities, which will compose the candidate set for the given entity.

\cite{Gillick2019} and subsequent research done with a similar approach, \cite{Wu2020}, embedded the entities and the mentions in the same dense vector space. For that, they used two separate bi-encoder models one to generate entity embeddings and one for the mentions. In all three works, similar inputs for encoding the mentions and the entities have been adopted. Namely; for the mentions, words from a context window and the mention itself have been passed to the bi-encoder, and as for the entity, the title and description have been used as input. However, in each setup, the scoring function for the mention-entity pairs differed.\newline
In \cite{Wu2020} they optimized the scores of the correct pairs, by maximizing the dot product of both embeddings relative to the entities in the same batch. \cite{Gillick2019} scored the entity mention pairs by their cosine similarity.

\section{Summary}
In summary, many approaches and techniques have been presented over the years to solve this one task of entity linking. At the time of this work, systems that utilize embeddings to solve this problem have proven to be the most efficient \footnote{https://paperswithcode.com/sota/entity-linking-on-aida-conll}. In this work, we follow this stream of research and try to improve on it. We try to achieve this firstly by building a simple system, that does not rely on dictionaries for mentions, secondly by utilizing pre-computed knowledge graph embeddings, and finally by feeding more information rich input to models so that better predictions can be reached.

%Under the first category, which is a frequently adopted approach [NeuPL: Attention-based semantic matching] falls the work of [NeuPL: Attention-based semantic matching, ELwKB: Issues, Techniques, and Solutions].\newline
%First of all, a dictionary or index is compiled for a large number of entities. These data structures hold for many entities, present in the knowledge base, various surface forms. In [NeuPL: Attention-based semantic matching] This step is done using entity titles, anchor texts, redirect pages, and disambiguation pages in Wikipedia. Then for each mention in the input text, a search is executed to find entities in the dictionary that match its surface form.\newline
%If this primary search does not yield enough or any candidate entities, many proposed Entity Linking systems that use this technique for generating candidates, implement as well methods that extend the mention's form to, known aliases [ELwKB: Issues, Techniques, and Solutions, NeuPL: Attention-based semantic matching, AGDISTIS, A two-stage method to EL, ...].\newline
%For example, in [AGDISTIS], they use various NLP techniques like string normalization and expansion policies. For a given mention genitive and plural forms are removed as well as affixes and postfixed. A 4\% accuracy increase has been reported after the use of such techniques. Seeing that this method increases the number of candidates, and consequently the runtime, a pruning 
%of the candidate's set is done after the latter has been generated. In [AGDISTIS] this consisted in limiting the number of domains recognized mentions will be mapped to.\newline
%- AGDISTIS approach of NLP preprocessing techniques lemmatization and form expansion to generate enough candidates. This approach increases the accuracy of the system by 4\%. Seeing that this method increases the number of candidates in [AGDISTIS] they limit the number of domains the mentions will be mapped to, to avoid any decrease in the performance of the system. 
%In [NeuPL] the surface form expansion is done with the help of an off-the-shelf Named Entity Recognizer, and to resolve the problem of the candidate set's growing size they train a Gradient Boosted Regression Trees model as a candidate ranker. This model accounts for the prior probabilities of a given mention to map to a certain entity based on hyperlinks in Wikipedia, as well as String similarities.\newline

%Taking into consideration the methods presented above, we can safely conclude that the common approach does not fall under just one category of the above-named methods but rather under a combination of two or more categories. 
%These methodologies named have improved some aspects of the Entity Linking systems, however, they present some problems. First, to solve the lack of candidates, expansions and aliases were introduced, these have increased both the size of the indexes precomputed and the size of candidate sets. The subsequent solution to resolve such issues i.e. candidate sets pruning be it statistical or of other nature has decreased the runtime but it was done at the slight cost of accuracy[High-Quality CG].
%Moreover, to implement methods that consider co-occurrences probability statistical data is required. However, this data is scarce and has a large memory footprint [Deep joint ED with local neural attention].
%On top of the mentioned issues, some aspects are not at all taken into account, these are the semantic relevance of the mention and the context in which the mention is detected. [NLPR\_KBP in TAC] includes this information by submitting a short context window with the entity's mention to the Google API, and retrieving Wikipedia web pages.\newline

%A new way to take into consideration more information than just the surface forms of mentions and statistics of co-occurrences; is the reliance on Neural Network models to compute vector representations for entities and mentions in the same vector space. [Yamada, Francis-Landau, et al. 2016] have introduced this approach to Entity Linking, they focus its use solely on the Entity Disambiguation step. These works have inspired others to introduce the use of embeddings to the first step of EL; namely Candidate Generation as well [Gillick et al. 2019, Wu et al. 2020, Anonymous submission]. All these works have reported an increase in accuracy and recall and a decrease in runtime.

